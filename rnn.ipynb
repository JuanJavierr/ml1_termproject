{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5a0750-f2b9-4fa0-8a80-d9782bcb5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from modules.preprocess import *\n",
    "from modules.utils import build_dataset, text_to_word2vec, evaluate\n",
    "from modules.rnn_model import TextRNN\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06794c9c-55f3-4acd-9a85-4a24994f99d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3288cc5c-c2b0-47ad-b026-c8c865d0993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/anaconda-2/lapresse_crawler/modules/utils.py:50: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  .str.replace(\"\\n\\d\\n\", \"\")\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(path=\"lapresse_crawler\", num_samples=20000, rnd_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b2002e-03bd-4576-9aba-456b44dc4bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12377/12377 [00:03<00:00, 3585.65it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = text_edit(dataset, grp_num=True, rm_newline=True, rm_punctuation=True,\n",
    "              rm_stop_words=False, lowercase=True, lemmatize=False, html_=True, convert_entities=False, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a668781-b2e5-4a2f-941c-ea9171de7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x['text'] for x in dataset.values() if x['section_1'] in INCLUDED_SECTIONS]\n",
    "Y = [x['section_label'] for x in dataset.values() if x['section_1'] in INCLUDED_SECTIONS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7334047b-0c39-4e94-b048-19c2b275afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b37cdc4a-a1c4-4f8a-ab5b-91fe10d53e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fasttext-wiki-news-subwords-300'  \n",
    "word2vec_model = api.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f567452e-88d9-47c1-80f7-b7de421a770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ceci est un texte exemple\"\n",
    "vector = text_to_word2vec(text, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f24c2cc5-9891-43cb-974c-a9ff0ec5d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vector.shape[0]  \n",
    "hidden_size = 256\n",
    "output_size = len(set(Y_train))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "887c529e-3d40-46e9-a166-a3a89f5f0de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc4afb-30ab-4e74-91fe-2f2af9bc11f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4db3f94d-d97b-429e-8464-dc8027dbeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.stack([torch.tensor(text_to_word2vec(x, word2vec_model), dtype=torch.float32).view(1,-1) for x in X_train], dim=0)\n",
    "X_test = torch.stack([torch.tensor(text_to_word2vec(x, word2vec_model), dtype=torch.float32).view(1,-1) for x in X_test], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f32d16af-cc74-442e-bc5a-987f5b08f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747dc967-788f-41fe-9211-8e79bae7ffb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "632e9e2f-801b-48b9-a4f9-9e2fecc3c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for epoch 0:\n",
      "Mean train loss for epoch: 1.4543715715408325\n",
      "Mean test loss for epoch: 1.354439616203308\n",
      "Model saved at epoch 0 with test loss 1.354439616203308\n",
      "Results for epoch 1:\n",
      "Mean train loss for epoch: 1.2950513362884521\n",
      "Mean test loss for epoch: 1.2250109910964966\n",
      "Model saved at epoch 1 with test loss 1.2250109910964966\n",
      "Results for epoch 2:\n",
      "Mean train loss for epoch: 1.2150484323501587\n",
      "Mean test loss for epoch: 1.178482174873352\n",
      "Model saved at epoch 2 with test loss 1.178482174873352\n",
      "Results for epoch 3:\n",
      "Mean train loss for epoch: 1.1744625568389893\n",
      "Mean test loss for epoch: 1.1677756309509277\n",
      "Model saved at epoch 3 with test loss 1.1677756309509277\n",
      "Results for epoch 4:\n",
      "Mean train loss for epoch: 1.1323598623275757\n",
      "Mean test loss for epoch: 1.113921046257019\n",
      "Model saved at epoch 4 with test loss 1.113921046257019\n",
      "Results for epoch 5:\n",
      "Mean train loss for epoch: 1.1090340614318848\n",
      "Mean test loss for epoch: 1.1303430795669556\n",
      "Results for epoch 6:\n",
      "Mean train loss for epoch: 1.097739815711975\n",
      "Mean test loss for epoch: 1.1223642826080322\n",
      "Results for epoch 7:\n",
      "Mean train loss for epoch: 1.0904510021209717\n",
      "Mean test loss for epoch: 1.106916069984436\n",
      "Model saved at epoch 7 with test loss 1.106916069984436\n",
      "Results for epoch 8:\n",
      "Mean train loss for epoch: 1.0809568166732788\n",
      "Mean test loss for epoch: 1.081655740737915\n",
      "Model saved at epoch 8 with test loss 1.081655740737915\n",
      "Results for epoch 9:\n",
      "Mean train loss for epoch: 1.0819543600082397\n",
      "Mean test loss for epoch: 1.0815150737762451\n",
      "Model saved at epoch 9 with test loss 1.0815150737762451\n",
      "Results for epoch 10:\n",
      "Mean train loss for epoch: 1.077064871788025\n",
      "Mean test loss for epoch: 1.0886913537979126\n",
      "Results for epoch 11:\n",
      "Mean train loss for epoch: 1.0739150047302246\n",
      "Mean test loss for epoch: 1.0783874988555908\n",
      "Model saved at epoch 11 with test loss 1.0783874988555908\n",
      "Results for epoch 12:\n",
      "Mean train loss for epoch: 1.0693774223327637\n",
      "Mean test loss for epoch: 1.0773320198059082\n",
      "Model saved at epoch 12 with test loss 1.0773320198059082\n",
      "Results for epoch 13:\n",
      "Mean train loss for epoch: 1.0669759511947632\n",
      "Mean test loss for epoch: 1.089616298675537\n",
      "Results for epoch 14:\n",
      "Mean train loss for epoch: 1.0663964748382568\n",
      "Mean test loss for epoch: 1.0997278690338135\n",
      "Results for epoch 15:\n",
      "Mean train loss for epoch: 1.0639901161193848\n",
      "Mean test loss for epoch: 1.0582053661346436\n",
      "Model saved at epoch 15 with test loss 1.0582053661346436\n",
      "Results for epoch 16:\n",
      "Mean train loss for epoch: 1.063252568244934\n",
      "Mean test loss for epoch: 1.07743239402771\n",
      "Results for epoch 17:\n",
      "Mean train loss for epoch: 1.0602104663848877\n",
      "Mean test loss for epoch: 1.0893614292144775\n",
      "Results for epoch 18:\n",
      "Mean train loss for epoch: 1.0553100109100342\n",
      "Mean test loss for epoch: 1.0937185287475586\n",
      "Results for epoch 19:\n",
      "Mean train loss for epoch: 1.0588798522949219\n",
      "Mean test loss for epoch: 1.0532389879226685\n",
      "Model saved at epoch 19 with test loss 1.0532389879226685\n",
      "Results for epoch 20:\n",
      "Mean train loss for epoch: 1.0553526878356934\n",
      "Mean test loss for epoch: 1.083387017250061\n",
      "Results for epoch 21:\n",
      "Mean train loss for epoch: 1.058906078338623\n",
      "Mean test loss for epoch: 1.078199863433838\n",
      "Results for epoch 22:\n",
      "Mean train loss for epoch: 1.0553479194641113\n",
      "Mean test loss for epoch: 1.0475631952285767\n",
      "Model saved at epoch 22 with test loss 1.0475631952285767\n",
      "Results for epoch 23:\n",
      "Mean train loss for epoch: 1.0550522804260254\n",
      "Mean test loss for epoch: 1.0913047790527344\n",
      "Results for epoch 24:\n",
      "Mean train loss for epoch: 1.0523267984390259\n",
      "Mean test loss for epoch: 1.0712248086929321\n",
      "Results for epoch 25:\n",
      "Mean train loss for epoch: 1.048397421836853\n",
      "Mean test loss for epoch: 1.049901008605957\n",
      "Results for epoch 26:\n",
      "Mean train loss for epoch: 1.050878882408142\n",
      "Mean test loss for epoch: 1.0907096862792969\n",
      "Results for epoch 27:\n",
      "Mean train loss for epoch: 1.0505739450454712\n",
      "Mean test loss for epoch: 1.0624860525131226\n",
      "Results for epoch 28:\n",
      "Mean train loss for epoch: 1.0505565404891968\n",
      "Mean test loss for epoch: 1.064405918121338\n",
      "Results for epoch 29:\n",
      "Mean train loss for epoch: 1.0481643676757812\n",
      "Mean test loss for epoch: 1.0890494585037231\n",
      "Results for epoch 30:\n",
      "Mean train loss for epoch: 1.043932318687439\n",
      "Mean test loss for epoch: 1.0680162906646729\n",
      "Results for epoch 31:\n",
      "Mean train loss for epoch: 1.0436748266220093\n",
      "Mean test loss for epoch: 1.0461220741271973\n",
      "Model saved at epoch 31 with test loss 1.0461220741271973\n",
      "Results for epoch 32:\n",
      "Mean train loss for epoch: 1.0479367971420288\n",
      "Mean test loss for epoch: 1.0703907012939453\n",
      "Results for epoch 33:\n",
      "Mean train loss for epoch: 1.0421911478042603\n",
      "Mean test loss for epoch: 1.0513346195220947\n",
      "Results for epoch 34:\n",
      "Mean train loss for epoch: 1.044979453086853\n",
      "Mean test loss for epoch: 1.0446746349334717\n",
      "Model saved at epoch 34 with test loss 1.0446746349334717\n",
      "Results for epoch 35:\n",
      "Mean train loss for epoch: 1.0475255250930786\n",
      "Mean test loss for epoch: 1.0386282205581665\n",
      "Model saved at epoch 35 with test loss 1.0386282205581665\n",
      "Results for epoch 36:\n",
      "Mean train loss for epoch: 1.043426513671875\n",
      "Mean test loss for epoch: 1.0531448125839233\n",
      "Results for epoch 37:\n",
      "Mean train loss for epoch: 1.0427217483520508\n",
      "Mean test loss for epoch: 1.1129435300827026\n",
      "Results for epoch 38:\n",
      "Mean train loss for epoch: 1.0407618284225464\n",
      "Mean test loss for epoch: 1.0619498491287231\n",
      "Results for epoch 39:\n",
      "Mean train loss for epoch: 1.0429251194000244\n",
      "Mean test loss for epoch: 1.0638718605041504\n",
      "Results for epoch 40:\n",
      "Mean train loss for epoch: 1.0369819402694702\n",
      "Mean test loss for epoch: 1.087424874305725\n",
      "Results for epoch 41:\n",
      "Mean train loss for epoch: 1.0418858528137207\n",
      "Mean test loss for epoch: 1.0736236572265625\n",
      "Results for epoch 42:\n",
      "Mean train loss for epoch: 1.041565179824829\n",
      "Mean test loss for epoch: 1.059762716293335\n",
      "Results for epoch 43:\n",
      "Mean train loss for epoch: 1.0381109714508057\n",
      "Mean test loss for epoch: 1.0413014888763428\n",
      "Results for epoch 44:\n",
      "Mean train loss for epoch: 1.036134958267212\n",
      "Mean test loss for epoch: 1.0869709253311157\n",
      "Results for epoch 45:\n",
      "Mean train loss for epoch: 1.0363823175430298\n",
      "Mean test loss for epoch: 1.0468775033950806\n",
      "Results for epoch 46:\n",
      "Mean train loss for epoch: 1.0400158166885376\n",
      "Mean test loss for epoch: 1.0480319261550903\n",
      "Results for epoch 47:\n",
      "Mean train loss for epoch: 1.0388704538345337\n",
      "Mean test loss for epoch: 1.0504350662231445\n",
      "Results for epoch 48:\n",
      "Mean train loss for epoch: 1.0382853746414185\n",
      "Mean test loss for epoch: 1.0554033517837524\n",
      "Results for epoch 49:\n",
      "Mean train loss for epoch: 1.0363788604736328\n",
      "Mean test loss for epoch: 1.0474402904510498\n",
      "Results for epoch 50:\n",
      "Mean train loss for epoch: 1.0349528789520264\n",
      "Mean test loss for epoch: 1.0577236413955688\n",
      "Results for epoch 51:\n",
      "Mean train loss for epoch: 1.0386555194854736\n",
      "Mean test loss for epoch: 1.0410642623901367\n",
      "Results for epoch 52:\n",
      "Mean train loss for epoch: 1.0335372686386108\n",
      "Mean test loss for epoch: 1.0622578859329224\n",
      "Results for epoch 53:\n",
      "Mean train loss for epoch: 1.034598469734192\n",
      "Mean test loss for epoch: 1.042940616607666\n",
      "Results for epoch 54:\n",
      "Mean train loss for epoch: 1.0321073532104492\n",
      "Mean test loss for epoch: 1.0362635850906372\n",
      "Model saved at epoch 54 with test loss 1.0362635850906372\n",
      "Results for epoch 55:\n",
      "Mean train loss for epoch: 1.0327831506729126\n",
      "Mean test loss for epoch: 1.0374208688735962\n",
      "Results for epoch 56:\n",
      "Mean train loss for epoch: 1.0321500301361084\n",
      "Mean test loss for epoch: 1.047640323638916\n",
      "Results for epoch 57:\n",
      "Mean train loss for epoch: 1.0312793254852295\n",
      "Mean test loss for epoch: 1.0625011920928955\n",
      "Results for epoch 58:\n",
      "Mean train loss for epoch: 1.0321027040481567\n",
      "Mean test loss for epoch: 1.0342283248901367\n",
      "Model saved at epoch 58 with test loss 1.0342283248901367\n",
      "Results for epoch 59:\n",
      "Mean train loss for epoch: 1.0340074300765991\n",
      "Mean test loss for epoch: 1.0428366661071777\n",
      "Results for epoch 60:\n",
      "Mean train loss for epoch: 1.032426118850708\n",
      "Mean test loss for epoch: 1.0317566394805908\n",
      "Model saved at epoch 60 with test loss 1.0317566394805908\n",
      "Results for epoch 61:\n",
      "Mean train loss for epoch: 1.0339113473892212\n",
      "Mean test loss for epoch: 1.0631959438323975\n",
      "Results for epoch 62:\n",
      "Mean train loss for epoch: 1.029920220375061\n",
      "Mean test loss for epoch: 1.0428669452667236\n",
      "Results for epoch 63:\n",
      "Mean train loss for epoch: 1.0293911695480347\n",
      "Mean test loss for epoch: 1.047844648361206\n",
      "Results for epoch 64:\n",
      "Mean train loss for epoch: 1.0308130979537964\n",
      "Mean test loss for epoch: 1.0338709354400635\n",
      "Results for epoch 65:\n",
      "Mean train loss for epoch: 1.0356279611587524\n",
      "Mean test loss for epoch: 1.0474128723144531\n",
      "Results for epoch 66:\n",
      "Mean train loss for epoch: 1.0346325635910034\n",
      "Mean test loss for epoch: 1.0673973560333252\n",
      "Results for epoch 67:\n",
      "Mean train loss for epoch: 1.030268907546997\n",
      "Mean test loss for epoch: 1.0401556491851807\n",
      "Results for epoch 68:\n",
      "Mean train loss for epoch: 1.0289291143417358\n",
      "Mean test loss for epoch: 1.0686579942703247\n",
      "Results for epoch 69:\n",
      "Mean train loss for epoch: 1.0305347442626953\n",
      "Mean test loss for epoch: 1.043993353843689\n",
      "Results for epoch 70:\n",
      "Mean train loss for epoch: 1.0300148725509644\n",
      "Mean test loss for epoch: 1.042807698249817\n",
      "Results for epoch 71:\n",
      "Mean train loss for epoch: 1.031239628791809\n",
      "Mean test loss for epoch: 1.067253589630127\n",
      "Results for epoch 72:\n",
      "Mean train loss for epoch: 1.0287158489227295\n",
      "Mean test loss for epoch: 1.056230068206787\n",
      "Results for epoch 73:\n",
      "Mean train loss for epoch: 1.0296173095703125\n",
      "Mean test loss for epoch: 1.039319634437561\n",
      "Results for epoch 74:\n",
      "Mean train loss for epoch: 1.0276445150375366\n",
      "Mean test loss for epoch: 1.0679559707641602\n",
      "Results for epoch 75:\n",
      "Mean train loss for epoch: 1.0259610414505005\n",
      "Mean test loss for epoch: 1.0370796918869019\n",
      "Results for epoch 76:\n",
      "Mean train loss for epoch: 1.0301779508590698\n",
      "Mean test loss for epoch: 1.0576800107955933\n",
      "Results for epoch 77:\n",
      "Mean train loss for epoch: 1.0276795625686646\n",
      "Mean test loss for epoch: 1.032218337059021\n",
      "Results for epoch 78:\n",
      "Mean train loss for epoch: 1.03215491771698\n",
      "Mean test loss for epoch: 1.0427796840667725\n",
      "Results for epoch 79:\n",
      "Mean train loss for epoch: 1.0287381410598755\n",
      "Mean test loss for epoch: 1.062813401222229\n",
      "Results for epoch 80:\n",
      "Mean train loss for epoch: 1.0315500497817993\n",
      "Mean test loss for epoch: 1.0427637100219727\n",
      "Results for epoch 81:\n",
      "Mean train loss for epoch: 1.0248258113861084\n",
      "Mean test loss for epoch: 1.0639532804489136\n",
      "Results for epoch 82:\n",
      "Mean train loss for epoch: 1.0279899835586548\n",
      "Mean test loss for epoch: 1.0457367897033691\n",
      "Results for epoch 83:\n",
      "Mean train loss for epoch: 1.0279548168182373\n",
      "Mean test loss for epoch: 1.031489610671997\n",
      "Model saved at epoch 83 with test loss 1.031489610671997\n",
      "Results for epoch 84:\n",
      "Mean train loss for epoch: 1.0252721309661865\n",
      "Mean test loss for epoch: 1.0445036888122559\n",
      "Results for epoch 85:\n",
      "Mean train loss for epoch: 1.0250557661056519\n",
      "Mean test loss for epoch: 1.0368283987045288\n",
      "Results for epoch 86:\n",
      "Mean train loss for epoch: 1.027160882949829\n",
      "Mean test loss for epoch: 1.0442705154418945\n",
      "Results for epoch 87:\n",
      "Mean train loss for epoch: 1.025617241859436\n",
      "Mean test loss for epoch: 1.0477209091186523\n",
      "Results for epoch 88:\n",
      "Mean train loss for epoch: 1.0262596607208252\n",
      "Mean test loss for epoch: 1.0285696983337402\n",
      "Model saved at epoch 88 with test loss 1.0285696983337402\n",
      "Results for epoch 89:\n",
      "Mean train loss for epoch: 1.0252208709716797\n",
      "Mean test loss for epoch: 1.043948769569397\n",
      "Results for epoch 90:\n",
      "Mean train loss for epoch: 1.0254141092300415\n",
      "Mean test loss for epoch: 1.031377911567688\n",
      "Results for epoch 91:\n",
      "Mean train loss for epoch: 1.0282820463180542\n",
      "Mean test loss for epoch: 1.041817545890808\n",
      "Results for epoch 92:\n",
      "Mean train loss for epoch: 1.0248119831085205\n",
      "Mean test loss for epoch: 1.044381856918335\n",
      "Results for epoch 93:\n",
      "Mean train loss for epoch: 1.025754451751709\n",
      "Mean test loss for epoch: 1.0616106986999512\n",
      "Results for epoch 94:\n",
      "Mean train loss for epoch: 1.0230827331542969\n",
      "Mean test loss for epoch: 1.0550190210342407\n",
      "Results for epoch 95:\n",
      "Mean train loss for epoch: 1.0263735055923462\n",
      "Mean test loss for epoch: 1.0400019884109497\n",
      "Results for epoch 96:\n",
      "Mean train loss for epoch: 1.0247807502746582\n",
      "Mean test loss for epoch: 1.0639621019363403\n",
      "Results for epoch 97:\n",
      "Mean train loss for epoch: 1.0248156785964966\n",
      "Mean test loss for epoch: 1.0390576124191284\n",
      "Results for epoch 98:\n",
      "Mean train loss for epoch: 1.0243679285049438\n",
      "Mean test loss for epoch: 1.042600154876709\n",
      "Results for epoch 99:\n",
      "Mean train loss for epoch: 1.021156668663025\n",
      "Mean test loss for epoch: 1.032340407371521\n",
      "Results for epoch 100:\n",
      "Mean train loss for epoch: 1.0247278213500977\n",
      "Mean test loss for epoch: 1.0377670526504517\n",
      "Results for epoch 101:\n",
      "Mean train loss for epoch: 1.0236351490020752\n",
      "Mean test loss for epoch: 1.031662106513977\n",
      "Results for epoch 102:\n",
      "Mean train loss for epoch: 1.0220974683761597\n",
      "Mean test loss for epoch: 1.033132553100586\n",
      "Results for epoch 103:\n",
      "Mean train loss for epoch: 1.023616909980774\n",
      "Mean test loss for epoch: 1.0339958667755127\n",
      "Results for epoch 104:\n",
      "Mean train loss for epoch: 1.0200210809707642\n",
      "Mean test loss for epoch: 1.0379705429077148\n",
      "Results for epoch 105:\n",
      "Mean train loss for epoch: 1.0236424207687378\n",
      "Mean test loss for epoch: 1.032724142074585\n",
      "Results for epoch 106:\n",
      "Mean train loss for epoch: 1.0252854824066162\n",
      "Mean test loss for epoch: 1.0317758321762085\n",
      "Results for epoch 107:\n",
      "Mean train loss for epoch: 1.0226372480392456\n",
      "Mean test loss for epoch: 1.0527962446212769\n",
      "Results for epoch 108:\n",
      "Mean train loss for epoch: 1.0219939947128296\n",
      "Mean test loss for epoch: 1.0378668308258057\n",
      "Results for epoch 109:\n",
      "Mean train loss for epoch: 1.0216283798217773\n",
      "Mean test loss for epoch: 1.0419073104858398\n",
      "Results for epoch 110:\n",
      "Mean train loss for epoch: 1.0204147100448608\n",
      "Mean test loss for epoch: 1.0342763662338257\n",
      "Results for epoch 111:\n",
      "Mean train loss for epoch: 1.0209102630615234\n",
      "Mean test loss for epoch: 1.0556493997573853\n",
      "Results for epoch 112:\n",
      "Mean train loss for epoch: 1.0220919847488403\n",
      "Mean test loss for epoch: 1.0448594093322754\n",
      "Results for epoch 113:\n",
      "Mean train loss for epoch: 1.024649977684021\n",
      "Mean test loss for epoch: 1.0467485189437866\n",
      "Results for epoch 114:\n",
      "Mean train loss for epoch: 1.0262115001678467\n",
      "Mean test loss for epoch: 1.0515706539154053\n",
      "Results for epoch 115:\n",
      "Mean train loss for epoch: 1.0183331966400146\n",
      "Mean test loss for epoch: 1.0486265420913696\n",
      "Results for epoch 116:\n",
      "Mean train loss for epoch: 1.0207033157348633\n",
      "Mean test loss for epoch: 1.0308115482330322\n",
      "Results for epoch 117:\n",
      "Mean train loss for epoch: 1.0252207517623901\n",
      "Mean test loss for epoch: 1.037615180015564\n",
      "Results for epoch 118:\n",
      "Mean train loss for epoch: 1.0232428312301636\n",
      "Mean test loss for epoch: 1.0524797439575195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, Y)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m, in \u001b[0;36mTextRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     10\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.12/site-packages/torch/nn/modules/rnn.py:554\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 554\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    559\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m    560\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataset = TensorDataset(X_train, Y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "epochs = 150\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for X, Y in dataloader:  \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.detach())\n",
    "    for X, Y in test_dataloader:  \n",
    "        model.eval()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        test_losses.append(loss.detach())\n",
    "\n",
    "    mean_test_loss = np.mean(test_losses)\n",
    "    print(f'Results for epoch {epoch}:')\n",
    "    print(f'Mean train loss for epoch: {np.mean(train_losses)}')\n",
    "    print(f'Mean test loss for epoch: {mean_test_loss}')\n",
    "\n",
    "    if mean_test_loss < best_test_loss:\n",
    "        best_test_loss = mean_test_loss\n",
    "        torch.save(model.state_dict(), 'rnn_best.pt') \n",
    "        print(f'Model saved at epoch {epoch} with test loss {mean_test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffe83e-23dd-42fc-9134-e1ad2af6002c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eaf30533-35af-4a5b-adfa-039331fc822c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TextRNN(input_size, hidden_size, output_size).to(device)  \n",
    "state_dict = torch.load('rnn_best.pt', map_location=device)  \n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01cefe8-d8a4-4e30-adfa-14471bd305a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "098c85c9-899f-44a6-bbfd-e867319af700",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_outputs = []\n",
    "for tensor_ in X_test:\n",
    "    output = model(tensor_.view(1,1,-1))\n",
    "    pred_class = np.argmax(output.detach())\n",
    "    pred_outputs.append(int(pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e39947-d1ea-4490-8456-0077e3119dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b0f57a2-fc13-461c-9e55-a9aa249df501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8783577228409979\n",
      "Recall:  0.8780290791599353\n",
      "F1_score:  0.8778465764731235\n",
      "accuracy:  0.8780290791599353\n"
     ]
    }
   ],
   "source": [
    "evaluate(Y_test.numpy(), np.array(pred_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27df10a-269e-4ab6-8dcb-fbf3050ba477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33b36b-7f7f-4364-a0f4-23924c07cdf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
