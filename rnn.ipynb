{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b5a0750-f2b9-4fa0-8a80-d9782bcb5082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/xavier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gensim\n",
    "import nltk\n",
    "from modules.preprocess import *\n",
    "from modules.utils import *\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc19555-72b4-4dca-ac51-f8b75db369c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06794c9c-55f3-4acd-9a85-4a24994f99d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xavier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3288cc5c-c2b0-47ad-b026-c8c865d0993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(path=\"lapresse_crawler\", num_samples=10000, rnd_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c284619c-df9c-4194-994b-3b015862e0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b2002e-03bd-4576-9aba-456b44dc4bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:03<00:00, 2876.73it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = text_edit(dataset, grp_num=True, rm_newline=True, rm_punctuation=True,\n",
    "              rm_stop_words=False, lowercase=True, lemmatize=False, html_=True, convert_entities=False, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a668781-b2e5-4a2f-941c-ea9171de7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x['text'] for x in dataset.values() if x['section_1'] in ['actualites', 'sports', 'international']]\n",
    "Y = [x['section_label'] for x in dataset.values() if x['section_1'] in ['actualites', 'sports', 'international']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7334047b-0c39-4e94-b048-19c2b275afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec636ac9-7c42-4415-8017-b318176d968d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9326ea43-f893-4e2c-9dae-9277340eb064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e788687-7fdc-40f7-af2e-283abe26d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fasttext-wiki-news-subwords-300'  \n",
    "word2vec_model = api.load(model_name)\n",
    "\n",
    "def preprocess_text(text, language='french'):\n",
    "    return word_tokenize(text.lower(), language=language)\n",
    "\n",
    "def text_to_word2vec(text, model):\n",
    "    words = preprocess_text(text)\n",
    "    vectors = [model[word] for word in words if word in model]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977399d-18e3-43bb-9512-24730e6e8c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ce661-4adf-4b42-b34a-c7a1a289f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return self.softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c2cc5-9891-43cb-974c-a9ff0ec5d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vector.shape[0]  \n",
    "hidden_size = 128\n",
    "output_size = len(set(Y_train))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c529e-3d40-46e9-a166-a3a89f5f0de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc4afb-30ab-4e74-91fe-2f2af9bc11f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3f94d-d97b-429e-8464-dc8027dbeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.stack([torch.tensor(text_to_word2vec(x, word2vec_model), dtype=torch.float32).view(1,-1) for x in X_train], dim=0)\n",
    "X_test = torch.stack([torch.tensor(text_to_word2vec(x, word2vec_model), dtype=torch.float32).view(1,-1) for x in X_test], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d16af-cc74-442e-bc5a-987f5b08f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747dc967-788f-41fe-9211-8e79bae7ffb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e9e2f-801b-48b9-a4f9-9e2fecc3c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataset = TensorDataset(X_train, Y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "epochs = 150\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for X, Y in dataloader:  \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.detach())\n",
    "    for X, Y in test_dataloader:  \n",
    "        model.eval()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        test_losses.append(loss.detach())\n",
    "\n",
    "    mean_test_loss = np.mean(test_losses)\n",
    "    print(f'Results for epoch {epoch}:')\n",
    "    print(f'Mean train loss for epoch: {np.mean(train_losses)}')\n",
    "    print(f'Mean test loss for epoch: {mean_test_loss}')\n",
    "\n",
    "    if mean_test_loss < best_test_loss:\n",
    "        best_test_loss = mean_test_loss\n",
    "        torch.save(model.state_dict(), 'rnn_best.pt') \n",
    "        print(f'Model saved at epoch {epoch} with test loss {mean_test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffe83e-23dd-42fc-9134-e1ad2af6002c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf30533-35af-4a5b-adfa-039331fc822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TextRNN(input_size, hidden_size, output_size).to(device)  \n",
    "state_dict = torch.load('rnn_best.pt', map_location=device)  \n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01cefe8-d8a4-4e30-adfa-14471bd305a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c85c9-899f-44a6-bbfd-e867319af700",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_outputs = []\n",
    "for tensor_ in X_test:\n",
    "    output = model(tensor_.view(1,1,-1))\n",
    "    pred_class = np.argmax(output.detach())\n",
    "    pred_outputs.append(int(pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e39947-d1ea-4490-8456-0077e3119dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f57a2-fc13-461c-9e55-a9aa249df501",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(Y_test.numpy(), np.array(pred_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27df10a-269e-4ab6-8dcb-fbf3050ba477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33b36b-7f7f-4364-a0f4-23924c07cdf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
